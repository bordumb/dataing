# Pydantic AI Migration Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Replace fragile regex/JSON parsing in AnthropicClient with Pydantic AI structured outputs for type-safe, self-validating LLM responses.

**Architecture:** Create response models with field validators that enforce quality (LIMIT clauses, no mutations, min lengths). Configure Pydantic AI agents per task type. Delete all manual parsing code and YAML templates.

**Tech Stack:** pydantic-ai, pydantic (existing), anthropic (existing)

---

## Task 1: Add pydantic-ai Dependency

**Files:**
- Modify: `pyproject.toml:9-33`

**Step 1: Add dependency**

Add `pydantic-ai>=0.0.14` to the dependencies list in pyproject.toml:

```toml
dependencies = [
    "fastapi[standard]>=0.109.0",
    "uvicorn[standard]>=0.27.0",
    "pydantic[email]>=2.5.0",
    "pydantic-ai>=0.0.14",
    # ... rest unchanged
]
```

**Step 2: Sync dependencies**

Run: `uv sync`
Expected: pydantic-ai and dependencies installed

**Step 3: Verify import works**

Run: `uv run python -c "from pydantic_ai import Agent; print('OK')"`
Expected: `OK`

**Step 4: Commit**

```bash
git add pyproject.toml uv.lock
git commit -m "chore: add pydantic-ai dependency"
```

---

## Task 2: Create Response Models

**Files:**
- Create: `backend/src/dataing/adapters/llm/response_models.py`

**Step 1: Create response_models.py with all models**

```python
"""Structured response models for LLM outputs.

These models define the exact schema expected from the LLM.
Pydantic AI uses these for:
1. Generating schema hints in the prompt
2. Validating LLM responses
3. Automatic retry on validation failure
"""

from __future__ import annotations

from pydantic import BaseModel, Field, field_validator

from dataing.core.domain_types import HypothesisCategory


class HypothesisResponse(BaseModel):
    """Single hypothesis from the LLM."""

    id: str = Field(description="Unique identifier like 'h1', 'h2', etc.")
    title: str = Field(
        description="Short, specific title describing the potential cause",
        min_length=10,
        max_length=200,
    )
    category: HypothesisCategory = Field(
        description="Classification of the hypothesis type"
    )
    reasoning: str = Field(
        description="Explanation of why this could be the cause",
        min_length=20,
    )
    suggested_query: str = Field(
        description="SQL query to investigate this hypothesis. Must include LIMIT clause.",
    )

    @field_validator("suggested_query")
    @classmethod
    def validate_query_has_limit(cls, v: str) -> str:
        """Ensure query has LIMIT clause for safety."""
        if "LIMIT" not in v.upper():
            raise ValueError("Query must include LIMIT clause")
        return v

    @field_validator("suggested_query")
    @classmethod
    def validate_no_mutations(cls, v: str) -> str:
        """Ensure query is read-only."""
        dangerous = ["INSERT", "UPDATE", "DELETE", "DROP", "TRUNCATE", "ALTER"]
        upper_query = v.upper()
        for keyword in dangerous:
            if keyword in upper_query:
                raise ValueError(f"Query contains forbidden keyword: {keyword}")
        return v


class HypothesesResponse(BaseModel):
    """Container for multiple hypotheses."""

    hypotheses: list[HypothesisResponse] = Field(
        description="List of hypotheses to investigate",
        min_length=1,
        max_length=10,
    )


class QueryResponse(BaseModel):
    """SQL query generated by LLM."""

    query: str = Field(description="The SQL query to execute")
    explanation: str = Field(
        description="Brief explanation of what the query tests",
        default="",
    )

    @field_validator("query")
    @classmethod
    def validate_query(cls, v: str) -> str:
        """Validate the generated SQL."""
        # Strip markdown if present
        if v.startswith("```"):
            lines = v.strip().split("\n")
            v = "\n".join(lines[1:-1] if lines[-1] == "```" else lines[1:])

        upper_query = v.upper().strip()

        if not upper_query.startswith("SELECT"):
            raise ValueError("Query must be a SELECT statement")

        if "LIMIT" not in upper_query:
            raise ValueError("Query must include LIMIT clause")

        return v.strip()


class InterpretationResponse(BaseModel):
    """LLM interpretation of query results."""

    supports_hypothesis: bool | None = Field(
        description="True if evidence supports, False if refutes, None if inconclusive"
    )
    confidence: float = Field(
        ge=0.0,
        le=1.0,
        description="Confidence score from 0.0 (no confidence) to 1.0 (certain)",
    )
    interpretation: str = Field(
        description="Human-readable explanation of what the results show",
        min_length=20,
    )
    key_findings: list[str] = Field(
        default_factory=list,
        description="Bullet points of the most important findings",
        max_length=5,
    )


class SynthesisResponse(BaseModel):
    """Final synthesis of all evidence into a finding."""

    root_cause: str | None = Field(
        description="Concise description of the root cause, or null if inconclusive"
    )
    confidence: float = Field(
        ge=0.0,
        le=1.0,
        description="Confidence in the root cause determination",
    )
    supporting_evidence: list[str] = Field(
        description="Key evidence points that support this conclusion",
        max_length=10,
    )
    recommendations: list[str] = Field(
        description="Actionable recommendations to fix or prevent the issue",
        min_length=1,
        max_length=5,
    )

    @field_validator("root_cause")
    @classmethod
    def validate_root_cause_quality(cls, v: str | None) -> str | None:
        """Ensure root cause is specific enough."""
        if v is not None and len(v) < 20:
            raise ValueError("Root cause description too vague (min 20 chars)")
        return v
```

**Step 2: Verify syntax**

Run: `PYTHONPATH=backend/src uv run python -c "from dataing.adapters.llm.response_models import HypothesesResponse; print('OK')"`
Expected: `OK`

**Step 3: Commit**

```bash
git add backend/src/dataing/adapters/llm/response_models.py
git commit -m "feat: add Pydantic AI response models with validators"
```

---

## Task 3: Write Response Model Tests

**Files:**
- Create: `tests/unit/adapters/llm/test_response_models.py`

**Step 1: Create test file**

```python
"""Tests for LLM response models validation."""

import pytest
from pydantic import ValidationError

from dataing.adapters.llm.response_models import (
    HypothesisResponse,
    HypothesesResponse,
    InterpretationResponse,
    QueryResponse,
    SynthesisResponse,
)
from dataing.core.domain_types import HypothesisCategory


class TestHypothesisResponse:
    """Tests for HypothesisResponse validation."""

    def test_valid_hypothesis(self) -> None:
        """Accept valid hypothesis."""
        h = HypothesisResponse(
            id="h1",
            title="Upstream table stg_orders has missing data",
            category=HypothesisCategory.UPSTREAM_DEPENDENCY,
            reasoning="The orders table depends on stg_orders which may have failed to load",
            suggested_query="SELECT COUNT(*) FROM analytics.stg_orders LIMIT 1000",
        )
        assert h.id == "h1"
        assert h.category == HypothesisCategory.UPSTREAM_DEPENDENCY

    def test_rejects_short_title(self) -> None:
        """Reject title that's too short."""
        with pytest.raises(ValidationError) as exc_info:
            HypothesisResponse(
                id="h1",
                title="Bad",
                category=HypothesisCategory.DATA_QUALITY,
                reasoning="Some reasoning here that is long enough",
                suggested_query="SELECT 1 LIMIT 1",
            )
        assert "title" in str(exc_info.value).lower()

    def test_rejects_query_without_limit(self) -> None:
        """Reject query missing LIMIT clause."""
        with pytest.raises(ValidationError) as exc_info:
            HypothesisResponse(
                id="h1",
                title="Valid title that is long enough for validation",
                category=HypothesisCategory.DATA_QUALITY,
                reasoning="Some reasoning here that is long enough",
                suggested_query="SELECT * FROM users",
            )
        assert "LIMIT" in str(exc_info.value)

    def test_rejects_mutation_query(self) -> None:
        """Reject query with mutation keywords."""
        with pytest.raises(ValidationError) as exc_info:
            HypothesisResponse(
                id="h1",
                title="Valid title that is long enough for validation",
                category=HypothesisCategory.DATA_QUALITY,
                reasoning="Some reasoning here that is long enough",
                suggested_query="DELETE FROM users LIMIT 100",
            )
        assert "DELETE" in str(exc_info.value)


class TestQueryResponse:
    """Tests for QueryResponse validation."""

    def test_strips_markdown(self) -> None:
        """Strip markdown code blocks from query."""
        q = QueryResponse(
            query="```sql\nSELECT * FROM users LIMIT 10\n```",
            explanation="Get users",
        )
        assert q.query == "SELECT * FROM users LIMIT 10"

    def test_rejects_non_select(self) -> None:
        """Reject non-SELECT queries."""
        with pytest.raises(ValidationError):
            QueryResponse(query="INSERT INTO users VALUES (1) LIMIT 1")

    def test_rejects_missing_limit(self) -> None:
        """Reject query without LIMIT."""
        with pytest.raises(ValidationError):
            QueryResponse(query="SELECT * FROM users")


class TestInterpretationResponse:
    """Tests for InterpretationResponse validation."""

    def test_confidence_upper_bound(self) -> None:
        """Reject confidence > 1.0."""
        with pytest.raises(ValidationError):
            InterpretationResponse(
                supports_hypothesis=True,
                confidence=1.5,
                interpretation="This is a valid interpretation text",
            )

    def test_confidence_lower_bound(self) -> None:
        """Reject confidence < 0.0."""
        with pytest.raises(ValidationError):
            InterpretationResponse(
                supports_hypothesis=True,
                confidence=-0.1,
                interpretation="This is a valid interpretation text",
            )

    def test_null_support_allowed(self) -> None:
        """Allow null for inconclusive evidence."""
        i = InterpretationResponse(
            supports_hypothesis=None,
            confidence=0.4,
            interpretation="Evidence is inconclusive for this hypothesis",
        )
        assert i.supports_hypothesis is None


class TestSynthesisResponse:
    """Tests for SynthesisResponse validation."""

    def test_rejects_vague_root_cause(self) -> None:
        """Reject root cause that's too short."""
        with pytest.raises(ValidationError):
            SynthesisResponse(
                root_cause="Bad data",
                confidence=0.8,
                supporting_evidence=["Evidence 1"],
                recommendations=["Fix it"],
            )

    def test_allows_null_root_cause(self) -> None:
        """Allow null root cause for inconclusive investigations."""
        s = SynthesisResponse(
            root_cause=None,
            confidence=0.3,
            supporting_evidence=[],
            recommendations=["Collect more data"],
        )
        assert s.root_cause is None

    def test_requires_recommendations(self) -> None:
        """Require at least one recommendation."""
        with pytest.raises(ValidationError):
            SynthesisResponse(
                root_cause=None,
                confidence=0.3,
                supporting_evidence=[],
                recommendations=[],
            )
```

**Step 2: Run tests**

Run: `PYTHONPATH=backend/src uv run pytest tests/unit/adapters/llm/test_response_models.py -v`
Expected: All tests pass

**Step 3: Commit**

```bash
git add tests/unit/adapters/llm/test_response_models.py
git commit -m "test: add response model validation tests"
```

---

## Task 4: Refactor AnthropicClient

**Files:**
- Rewrite: `backend/src/dataing/adapters/llm/client.py`

**Step 1: Replace client.py with Pydantic AI implementation**

```python
"""Anthropic Claude implementation with Pydantic AI structured outputs."""

from __future__ import annotations

from typing import TYPE_CHECKING

from pydantic_ai import Agent
from pydantic_ai.models.anthropic import AnthropicModel

from dataing.core.domain_types import (
    AnomalyAlert,
    Evidence,
    Finding,
    Hypothesis,
    InvestigationContext,
)
from dataing.core.exceptions import LLMError

from .response_models import (
    HypothesesResponse,
    InterpretationResponse,
    QueryResponse,
    SynthesisResponse,
)

if TYPE_CHECKING:
    from dataing.adapters.datasource.types import QueryResult, SchemaResponse


class AnthropicClient:
    """Anthropic Claude implementation with structured outputs.

    Uses Pydantic AI for type-safe, validated LLM responses.
    """

    def __init__(
        self,
        api_key: str,
        model: str = "claude-sonnet-4-20250514",
        max_retries: int = 3,
    ) -> None:
        """Initialize the Anthropic client.

        Args:
            api_key: Anthropic API key.
            model: Model to use.
            max_retries: Max retries on validation failure.
        """
        self._model = AnthropicModel(model, api_key=api_key)
        self._max_retries = max_retries

        # Pre-configure agents for each task
        self._hypothesis_agent: Agent[None, HypothesesResponse] = Agent(
            model=self._model,
            result_type=HypothesesResponse,
            retries=max_retries,
        )
        self._interpretation_agent: Agent[None, InterpretationResponse] = Agent(
            model=self._model,
            result_type=InterpretationResponse,
            retries=max_retries,
        )
        self._synthesis_agent: Agent[None, SynthesisResponse] = Agent(
            model=self._model,
            result_type=SynthesisResponse,
            retries=max_retries,
        )
        self._query_agent: Agent[None, QueryResponse] = Agent(
            model=self._model,
            result_type=QueryResponse,
            retries=max_retries,
        )

    async def generate_hypotheses(
        self,
        alert: AnomalyAlert,
        context: InvestigationContext,
        num_hypotheses: int = 5,
    ) -> list[Hypothesis]:
        """Generate hypotheses for an anomaly.

        Args:
            alert: The anomaly alert to investigate.
            context: Available schema and lineage context.
            num_hypotheses: Target number of hypotheses.

        Returns:
            List of validated Hypothesis objects.

        Raises:
            LLMError: If LLM call fails after retries.
        """
        system_prompt = self._build_hypothesis_system_prompt(num_hypotheses)
        user_prompt = self._build_hypothesis_user_prompt(alert, context)

        try:
            result = await self._hypothesis_agent.run(
                user_prompt,
                system_prompt=system_prompt,
            )

            return [
                Hypothesis(
                    id=h.id,
                    title=h.title,
                    category=h.category,
                    reasoning=h.reasoning,
                    suggested_query=h.suggested_query,
                )
                for h in result.data.hypotheses
            ]

        except Exception as e:
            raise LLMError(
                f"Hypothesis generation failed: {e}",
                retryable=False,
            ) from e

    async def generate_query(
        self,
        hypothesis: Hypothesis,
        schema: "SchemaResponse",
        previous_error: str | None = None,
    ) -> str:
        """Generate SQL query to test a hypothesis.

        Args:
            hypothesis: The hypothesis to test.
            schema: Available database schema.
            previous_error: Error from previous attempt (for reflexion).

        Returns:
            Validated SQL query string.

        Raises:
            LLMError: If query generation fails.
        """
        if previous_error:
            prompt = self._build_reflexion_prompt(hypothesis, schema, previous_error)
            system = self._build_reflexion_system_prompt(schema)
        else:
            prompt = self._build_query_prompt(hypothesis)
            system = self._build_query_system_prompt(schema)

        try:
            result = await self._query_agent.run(
                prompt,
                system_prompt=system,
            )
            return result.data.query

        except Exception as e:
            raise LLMError(
                f"Query generation failed: {e}",
                retryable=True,
            ) from e

    async def interpret_evidence(
        self,
        hypothesis: Hypothesis,
        query: str,
        results: "QueryResult",
    ) -> Evidence:
        """Interpret query results as evidence.

        Args:
            hypothesis: The hypothesis being tested.
            query: The query that was executed.
            results: The query results.

        Returns:
            Evidence with validated interpretation.
        """
        prompt = self._build_interpretation_prompt(hypothesis, query, results)
        system = self._build_interpretation_system_prompt()

        try:
            result = await self._interpretation_agent.run(
                prompt,
                system_prompt=system,
            )

            return Evidence(
                hypothesis_id=hypothesis.id,
                query=query,
                result_summary=results.to_summary(),
                row_count=results.row_count,
                supports_hypothesis=result.data.supports_hypothesis,
                confidence=result.data.confidence,
                interpretation=result.data.interpretation,
            )

        except Exception as e:
            # Return low-confidence evidence on failure rather than crashing
            return Evidence(
                hypothesis_id=hypothesis.id,
                query=query,
                result_summary=results.to_summary(),
                row_count=results.row_count,
                supports_hypothesis=None,
                confidence=0.3,
                interpretation=f"Interpretation failed: {e}",
            )

    async def synthesize_findings(
        self,
        alert: AnomalyAlert,
        evidence: list[Evidence],
    ) -> Finding:
        """Synthesize all evidence into a root cause finding.

        Args:
            alert: The original anomaly alert.
            evidence: All collected evidence.

        Returns:
            Finding with validated root cause and recommendations.

        Raises:
            LLMError: If synthesis fails.
        """
        prompt = self._build_synthesis_prompt(alert, evidence)
        system = self._build_synthesis_system_prompt()

        try:
            result = await self._synthesis_agent.run(
                prompt,
                system_prompt=system,
            )

            return Finding(
                investigation_id="",  # Set by orchestrator
                status="completed" if result.data.root_cause else "inconclusive",
                root_cause=result.data.root_cause,
                confidence=result.data.confidence,
                evidence=evidence,
                recommendations=result.data.recommendations,
                duration_seconds=0.0,  # Set by orchestrator
            )

        except Exception as e:
            raise LLMError(
                f"Synthesis failed: {e}",
                retryable=False,
            ) from e

    # -------------------------------------------------------------------------
    # Prompt Building Methods
    # -------------------------------------------------------------------------

    def _build_hypothesis_system_prompt(self, num_hypotheses: int) -> str:
        """Build system prompt for hypothesis generation."""
        return f"""You are a data quality investigator. Given an anomaly alert and database context,
generate {num_hypotheses} hypotheses about what could have caused the anomaly.

HYPOTHESIS CATEGORIES:
- upstream_dependency: Source table missing data, late arrival, schema change
- transformation_bug: ETL logic error, incorrect aggregation, wrong join
- data_quality: Nulls, duplicates, invalid values, schema drift
- infrastructure: Job failure, timeout, resource exhaustion
- expected_variance: Seasonality, holiday, known business event

Each hypothesis MUST:
1. Have a clear, specific title (10-200 characters)
2. Include reasoning for why this could be the cause (at least 20 characters)
3. Suggest a SQL query to investigate using ONLY provided schema tables
4. Include LIMIT clause in all queries
5. Use only SELECT statements (no mutations)

Generate diverse hypotheses covering multiple categories when plausible."""

    def _build_hypothesis_user_prompt(
        self,
        alert: AnomalyAlert,
        context: InvestigationContext,
    ) -> str:
        """Build user prompt for hypothesis generation."""
        lineage_section = ""
        if context.lineage:
            lineage_section = f"""
## Data Lineage
{context.lineage.to_prompt_string()}
"""

        return f"""## Anomaly Alert
- Dataset: {alert.dataset_id}
- Metric: {alert.metric_name}
- Expected: {alert.expected_value}
- Actual: {alert.actual_value}
- Deviation: {alert.deviation_pct}%
- Anomaly Date: {alert.anomaly_date}
- Severity: {alert.severity}

## Available Schema
{context.schema.to_prompt_string()}
{lineage_section}
Generate hypotheses to investigate this anomaly."""

    def _build_query_system_prompt(self, schema: "SchemaResponse") -> str:
        """Build system prompt for query generation."""
        return f"""You are a SQL expert generating investigative queries.

CRITICAL RULES:
1. Use ONLY tables from the schema: {schema.get_table_names()}
2. Use ONLY columns that exist in those tables
3. SELECT queries ONLY - no mutations
4. Always include LIMIT clause (max 10000)
5. Use fully qualified table names (schema.table)

SCHEMA:
{schema.to_prompt_string()}"""

    def _build_query_prompt(self, hypothesis: Hypothesis) -> str:
        """Build user prompt for query generation."""
        return f"""Generate a SQL query to test this hypothesis:

Hypothesis: {hypothesis.title}
Category: {hypothesis.category.value}
Reasoning: {hypothesis.reasoning}

Generate a query that would confirm or refute this hypothesis."""

    def _build_reflexion_system_prompt(self, schema: "SchemaResponse") -> str:
        """Build system prompt for reflexion (query correction)."""
        return f"""You are debugging a failed SQL query. Analyze the error and fix the query.

AVAILABLE SCHEMA:
{schema.to_prompt_string()}

COMMON FIXES:
- "column does not exist": Check column name spelling, use correct table
- "relation does not exist": Use fully qualified name (schema.table)
- "type mismatch": Cast values appropriately
- "syntax error": Check SQL syntax for the target database

CRITICAL: Only use tables and columns from the schema above."""

    def _build_reflexion_prompt(
        self,
        hypothesis: Hypothesis,
        schema: "SchemaResponse",
        previous_error: str,
    ) -> str:
        """Build user prompt for reflexion."""
        return f"""The previous query failed. Generate a corrected version.

ORIGINAL QUERY:
{hypothesis.suggested_query}

ERROR MESSAGE:
{previous_error}

HYPOTHESIS BEING TESTED:
{hypothesis.title}

Generate a corrected SQL query that avoids this error."""

    def _build_interpretation_system_prompt(self) -> str:
        """Build system prompt for evidence interpretation."""
        return """You are analyzing query results to determine if they support a hypothesis.

Provide:
1. Whether evidence supports (true), refutes (false), or is inconclusive (null)
2. Confidence score from 0.0 to 1.0
3. Brief interpretation explaining your assessment (at least 20 characters)
4. Key findings as bullet points (max 5)

Be objective and base your assessment solely on the data returned."""

    def _build_interpretation_prompt(
        self,
        hypothesis: Hypothesis,
        query: str,
        results: "QueryResult",
    ) -> str:
        """Build user prompt for interpretation."""
        return f"""HYPOTHESIS: {hypothesis.title}
REASONING: {hypothesis.reasoning}

QUERY EXECUTED:
{query}

RESULTS ({results.row_count} rows):
{results.to_summary()}

Analyze whether these results support or refute the hypothesis."""

    def _build_synthesis_system_prompt(self) -> str:
        """Build system prompt for synthesis."""
        return """You are synthesizing investigation findings to determine root cause.

Review all evidence and determine:
1. The most likely root cause (be specific, at least 20 characters, or null if inconclusive)
2. Confidence level (0.0-1.0)
3. Key supporting evidence
4. Recommended actions (1-5 actionable items)

CONFIDENCE GUIDELINES:
- 0.9+: Strong evidence with clear causation
- 0.7-0.9: Good evidence, likely correct
- 0.5-0.7: Some evidence, but uncertain
- <0.5: Weak evidence, inconclusive (set root_cause to null)"""

    def _build_synthesis_prompt(
        self,
        alert: AnomalyAlert,
        evidence: list[Evidence],
    ) -> str:
        """Build user prompt for synthesis."""
        evidence_text = "\n\n".join(
            [
                f"""### Hypothesis: {e.hypothesis_id}
- Query: {e.query[:200]}...
- Interpretation: {e.interpretation}
- Confidence: {e.confidence}
- Supports hypothesis: {e.supports_hypothesis}"""
                for e in evidence
            ]
        )

        return f"""## Original Anomaly
- Dataset: {alert.dataset_id}
- Metric: {alert.metric_name} deviated by {alert.deviation_pct}%
- Expected: {alert.expected_value}
- Actual: {alert.actual_value}
- Date: {alert.anomaly_date}

## Investigation Findings
{evidence_text}

Synthesize these findings into a root cause determination."""
```

**Step 2: Verify syntax**

Run: `PYTHONPATH=backend/src uv run python -c "from dataing.adapters.llm.client import AnthropicClient; print('OK')"`
Expected: `OK`

**Step 3: Commit**

```bash
git add backend/src/dataing/adapters/llm/client.py
git commit -m "feat: refactor AnthropicClient to use Pydantic AI agents"
```

---

## Task 5: Update Module Exports

**Files:**
- Modify: `backend/src/dataing/adapters/llm/__init__.py`

**Step 1: Update exports (remove PromptManager, add response models)**

```python
"""LLM adapter module."""

from .client import AnthropicClient
from .response_models import (
    HypothesesResponse,
    HypothesisResponse,
    InterpretationResponse,
    QueryResponse,
    SynthesisResponse,
)

__all__ = [
    "AnthropicClient",
    "HypothesesResponse",
    "HypothesisResponse",
    "InterpretationResponse",
    "QueryResponse",
    "SynthesisResponse",
]
```

**Step 2: Verify import**

Run: `PYTHONPATH=backend/src uv run python -c "from dataing.adapters.llm import AnthropicClient, HypothesesResponse; print('OK')"`
Expected: `OK`

**Step 3: Commit**

```bash
git add backend/src/dataing/adapters/llm/__init__.py
git commit -m "refactor: update llm module exports"
```

---

## Task 6: Delete Prompt Manager

**Files:**
- Delete: `backend/src/dataing/adapters/llm/prompt_manager.py`

**Step 1: Remove file**

```bash
rm backend/src/dataing/adapters/llm/prompt_manager.py
```

**Step 2: Verify module still imports**

Run: `PYTHONPATH=backend/src uv run python -c "from dataing.adapters.llm import AnthropicClient; print('OK')"`
Expected: `OK`

**Step 3: Commit**

```bash
git add -u backend/src/dataing/adapters/llm/prompt_manager.py
git commit -m "refactor: remove PromptManager (prompts now inline)"
```

---

## Task 7: Delete YAML Prompt Templates

**Files:**
- Delete: `backend/src/dataing/prompts/` (entire directory)

**Step 1: Remove directory**

```bash
rm -rf backend/src/dataing/prompts/
```

**Step 2: Verify no imports break**

Run: `PYTHONPATH=backend/src uv run python -c "from dataing.adapters.llm import AnthropicClient; print('OK')"`
Expected: `OK`

**Step 3: Commit**

```bash
git add -u backend/src/dataing/prompts/
git commit -m "refactor: remove YAML prompt templates (now inline in client)"
```

---

## Task 8: Run Linting

**Files:** All modified files

**Step 1: Run ruff**

Run: `uv run ruff check backend/src/dataing/adapters/llm/ --fix`
Expected: No errors (or auto-fixed)

**Step 2: Run ruff format**

Run: `uv run ruff format backend/src/dataing/adapters/llm/`
Expected: Files formatted

**Step 3: Run mypy**

Run: `uv run mypy backend/src/dataing/adapters/llm/`
Expected: No errors

**Step 4: Commit any fixes**

```bash
git add backend/src/dataing/adapters/llm/
git commit -m "style: fix linting issues" --allow-empty
```

---

## Task 9: Run Response Model Tests

**Files:** Test files

**Step 1: Run tests**

Run: `PYTHONPATH=backend/src uv run pytest tests/unit/adapters/llm/test_response_models.py -v`
Expected: All tests pass

**Step 2: Commit (if any test file changes)**

```bash
git add tests/unit/adapters/llm/
git commit -m "test: ensure response model tests pass" --allow-empty
```

---

## Task 10: Final Verification

**Step 1: Run full lint suite**

Run: `uv run ruff check backend/src/dataing/ && uv run mypy backend/src/dataing/adapters/llm/`
Expected: No errors

**Step 2: Verify clean git status**

Run: `git status`
Expected: Nothing to commit, working tree clean

**Step 3: List commits**

Run: `git log --oneline -10`
Expected: See all commits from this migration

---

## Summary

**Files Created:**
- `backend/src/dataing/adapters/llm/response_models.py` (~150 lines)
- `tests/unit/adapters/llm/test_response_models.py` (~120 lines)

**Files Modified:**
- `pyproject.toml` (add pydantic-ai)
- `backend/src/dataing/adapters/llm/__init__.py` (new exports)
- `backend/src/dataing/adapters/llm/client.py` (complete rewrite)

**Files Deleted:**
- `backend/src/dataing/adapters/llm/prompt_manager.py`
- `backend/src/dataing/prompts/hypothesis.yaml`
- `backend/src/dataing/prompts/interpretation.yaml`
- `backend/src/dataing/prompts/synthesis.yaml`
- `backend/src/dataing/prompts/query.yaml`
- `backend/src/dataing/prompts/reflexion.yaml`
