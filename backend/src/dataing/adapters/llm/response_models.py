"""Structured response models for LLM outputs.

These models define the exact schema expected from the LLM.
Pydantic AI uses these for:
1. Generating schema hints in the prompt
2. Validating LLM responses
3. Automatic retry on validation failure
"""

from __future__ import annotations

import re

from pydantic import BaseModel, Field, field_validator

from dataing.core.domain_types import HypothesisCategory


class HypothesisResponse(BaseModel):
    """Single hypothesis from the LLM."""

    id: str = Field(description="Unique identifier like 'h1', 'h2', etc.")
    title: str = Field(
        description="Short, specific title describing the potential cause",
        min_length=10,
        max_length=200,
    )
    category: HypothesisCategory = Field(description="Classification of the hypothesis type")
    reasoning: str = Field(
        description="Explanation of why this could be the cause",
        min_length=20,
    )
    suggested_query: str = Field(
        description="SQL query to investigate this hypothesis. Must include LIMIT clause.",
    )
    expected_if_true: str = Field(
        description="What results we expect if this hypothesis is correct",
        min_length=10,
    )
    expected_if_false: str = Field(
        description="What results we expect if this hypothesis is wrong",
        min_length=10,
    )

    @field_validator("suggested_query")
    @classmethod
    def validate_query_safety(cls, v: str) -> str:
        """Validate query safety: strip markdown, require LIMIT, block mutations."""
        # Strip markdown if present
        if v.startswith("```"):
            lines = v.strip().split("\n")
            v = "\n".join(lines[1:-1] if lines[-1] == "```" else lines[1:])

        upper_query = v.upper().strip()

        # Ensure query has LIMIT clause for safety
        if "LIMIT" not in upper_query:
            raise ValueError("Query must include LIMIT clause")

        # Ensure query is read-only using word boundary regex to avoid false positives
        dangerous = [
            "INSERT",
            "UPDATE",
            "DELETE",
            "DROP",
            "TRUNCATE",
            "ALTER",
            "CREATE",
            "MERGE",
            "GRANT",
            "REVOKE",
            "EXEC",
            "EXECUTE",
        ]
        pattern = r"\b(" + "|".join(dangerous) + r")\b"
        if re.search(pattern, upper_query):
            raise ValueError("Query contains forbidden SQL operation")

        return v.strip()


class HypothesesResponse(BaseModel):
    """Container for multiple hypotheses."""

    hypotheses: list[HypothesisResponse] = Field(
        description="List of hypotheses to investigate",
        min_length=1,
        max_length=10,
    )


class QueryResponse(BaseModel):
    """SQL query generated by LLM."""

    query: str = Field(description="The SQL query to execute")
    explanation: str = Field(
        description="Brief explanation of what the query tests",
        default="",
    )

    @field_validator("query")
    @classmethod
    def validate_query(cls, v: str) -> str:
        """Validate the generated SQL."""
        # Strip markdown if present
        if v.startswith("```"):
            lines = v.strip().split("\n")
            v = "\n".join(lines[1:-1] if lines[-1] == "```" else lines[1:])

        upper_query = v.upper().strip()

        if not upper_query.startswith("SELECT"):
            raise ValueError("Query must be a SELECT statement")

        if "LIMIT" not in upper_query:
            raise ValueError("Query must include LIMIT clause")

        return v.strip()


class InterpretationResponse(BaseModel):
    """LLM interpretation of query results.

    The causal_chain field forces the LLM to articulate cause-and-effect,
    not just confirm that an issue exists.
    """

    supports_hypothesis: bool | None = Field(
        description="True if evidence supports, False if refutes, None if inconclusive"
    )
    confidence: float = Field(
        ge=0.0,
        le=1.0,
        description="Confidence score from 0.0 (no confidence) to 1.0 (certain)",
    )
    interpretation: str = Field(
        description="What the results reveal about the ROOT CAUSE, not just the symptom",
        min_length=50,
    )
    causal_chain: str = Field(
        description=(
            "The cause-and-effect explanation: what upstream change led to this observation? "
            "Example: 'users ETL stopped at 03:14 -> stale table -> JOIN produces NULLs'"
        ),
        min_length=30,
    )
    key_findings: list[str] = Field(
        description="Specific findings with data points (counts, timestamps, table names)",
        min_length=1,
        max_length=5,
    )
    next_investigation_step: str | None = Field(
        default=None,
        description="If inconclusive: what query or check would help determine the root cause?",
    )


class SynthesisResponse(BaseModel):
    """Final synthesis of investigation findings.

    Requires structured causal chain and impact assessment,
    not just a root cause string.
    """

    root_cause: str | None = Field(
        description=(
            "The UPSTREAM cause, not the symptom. Must explain WHY. "
            "Example: 'users ETL job timed out at 03:14 UTC due to API rate limiting' "
            "NOT: 'NULL user_ids in orders table'"
        )
    )
    confidence: float = Field(
        ge=0.0,
        le=1.0,
        description="Confidence in root cause (0.9+=certain, 0.7-0.9=likely, <0.7=uncertain)",
    )
    causal_chain: list[str] = Field(
        description=(
            "Step-by-step from root cause to observed symptom. "
            "Example: ['API rate limit hit', 'users ETL job timeout', "
            "'users table stale after 03:14', 'orders JOIN produces NULLs']"
        ),
        min_length=2,
        max_length=6,
    )
    estimated_onset: str = Field(
        description="When the issue started (timestamp or relative time, e.g., '03:14 UTC')",
        min_length=5,
    )
    affected_scope: str = Field(
        description="Blast radius: what else is affected? (downstream tables, reports, consumers)",
        min_length=10,
    )
    supporting_evidence: list[str] = Field(
        description="Specific evidence with data points that supports this conclusion",
        min_length=1,
        max_length=10,
    )
    recommendations: list[str] = Field(
        description=(
            "Actionable recommendations with specific targets. "
            "Example: 'Re-run stg_users job: airflow trigger_dag stg_users --backfill' "
            "NOT: 'Investigate the issue'"
        ),
        min_length=1,
        max_length=5,
    )

    @field_validator("root_cause")
    @classmethod
    def validate_root_cause_quality(cls, v: str | None) -> str | None:
        """Ensure root cause is specific enough."""
        if v is not None and len(v) < 20:
            raise ValueError("Root cause description too vague (min 20 chars)")
        return v
